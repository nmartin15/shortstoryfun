# Testing Guide

## Running Unit Tests

### Prerequisites

Ensure you have pytest installed:

```bash
source venv/bin/activate
pip install pytest pytest-cov
```

### Run All Tests

```bash
# Run all tests
pytest tests/ -v

# Run with coverage report
pytest tests/ --cov=src/shortstory --cov-report=html --cov-report=term

# View HTML coverage report (opens in browser)
open htmlcov/index.html  # macOS
# or
xdg-open htmlcov/index.html  # Linux
```

### Run Specific Test Suites

```bash
# Database storage tests
pytest tests/test_db_storage.py -v

# LLM utility tests
pytest tests/test_llm.py -v

# Genre configuration tests
pytest tests/test_genres.py -v

# Export functionality tests
pytest tests/test_export.py -v

# Pipeline tests
pytest tests/test_pipeline.py -v

# Validation tests
pytest tests/test_validation.py -v

# Word count tests
pytest tests/test_word_count.py -v

# Schema validation tests (validates generated content structure)
pytest tests/test_story_schema_validation.py -v
```

### Run Tests by Category

```bash
# Run only tests that don't require external services
pytest tests/ -v -m "not slow"

# Run tests with specific markers
pytest tests/ -v -m "integration"
```

### Test Coverage Goals

- **Current Coverage**: ~60-70% (pipeline, validation, word count)
- **Target Coverage**: 85%+ for unit tests
- **Priority Areas**: Database storage, LLM utilities, genres, exports

For detailed coverage improvement plans and test enhancement recommendations, see **[TEST_COVERAGE_IMPROVEMENTS.md](TEST_COVERAGE_IMPROVEMENTS.md)**.

## Quick Test Options

### Option 1: Test Story Generation Directly (Recommended)

Test the pipeline without the web server:

```bash
source venv/bin/activate
python test_story_generation.py
```

This will:
- Test the full pipeline (premise → outline → scaffold → draft → revise)
- Use Google Gemini API to generate a story
- Show you the generated story
- Display word count and statistics

### Option 2: Test via Web Interface

1. **Start the web server:**
   ```bash
   source venv/bin/activate
   python app.py
   ```

2. **Open your browser:**
   ```
   http://localhost:5000
   ```

3. **Fill in the form:**
   - Select a genre
   - Enter a story idea
   - (Optional) Add character details
   - (Optional) Add a theme
   - Click "Generate Story"

4. **View the generated story** in the editor

### Option 3: Test API Endpoints

1. **Start the server** (in one terminal):
   ```bash
   source venv/bin/activate
   python app.py
   ```

2. **Test endpoints** (in another terminal):
   ```bash
   # Health check
   curl http://localhost:5000/api/health
   
   # Get genres
   curl http://localhost:5000/api/genres
   
   # Generate a story
   curl -X POST http://localhost:5000/api/generate \
     -H "Content-Type: application/json" \
     -d '{
       "idea": "A lighthouse keeper who collects lost voices",
       "character": {"name": "Mara", "description": "A lighthouse keeper"},
       "theme": "What happens to stories we never tell?",
       "genre": "Literary"
     }'
   ```

3. **Or use the comprehensive API test script:**
   ```bash
   python test_api.py
   ```

## What to Expect

### Successful Test Output

When everything works, you should see:
- ✅ Premise captured
- ✅ Outline generated  
- ✅ Scaffold created
- ✅ Draft generated (with word count)
- ✅ Revision complete
- A full story text generated by the LLM

### If LLM Fails

The pipeline will automatically fall back to template-based generation, so you'll still get a story (just simpler/placeholder text).

## Troubleshooting

### "API key not found"
- Check your `.env` file has `GOOGLE_API_KEY=your_key`
- Make sure you're in the project root when running

### "Model not found"
- The default model is `gemini-2.5-flash`
- You can change it in `.env`: `LLM_MODEL=gemini-2.5-pro` (for better quality)

### "Connection refused"
- Make sure the Flask server is running
- Check it's on port 5000: `lsof -i :5000`

### Story generation is slow
- This is normal - LLM generation takes 10-30 seconds
- The web interface shows a loading indicator

## Test Suites Overview

### Core Test Files

- **`tests/test_pipeline.py`**: Pipeline stage tests (premise, outline, scaffold, draft, revise)
- **`tests/test_validation.py`**: Distinctiveness and validation tests
- **`tests/test_word_count.py`**: Word counting functionality tests
- **`tests/test_outline_scaffold.py`**: Outline and scaffold generation tests
- **`tests/test_template_draft.py`**: Template-based draft generation tests
- **`tests/test_story_schema_validation.py`**: Schema validation tests using Pydantic models (StoryModel, PremiseModel, OutlineModel) to validate generated content structure (14 tests)

### New Test Suites (Priority 1 & 2)

- **`tests/test_db_storage.py`**: Database storage CRUD, pagination, transactions (30+ tests)
- **`tests/test_llm.py`**: LLM client, model validation, token counting (40+ tests)
- **`tests/test_genres.py`**: Genre configuration and retrieval tests (30+ tests)
- **`tests/test_export.py`**: Export functionality (PDF, Markdown, TXT, DOCX, EPUB) (30+ tests)
- **`tests/test_api_comprehensive.py`**: Comprehensive API endpoint tests with response validation (39 tests)

### Test Execution Tips

- Use `-v` flag for verbose output showing each test
- Use `-s` flag to see print statements: `pytest tests/ -v -s`
- Use `-k` to run tests matching a pattern: `pytest tests/ -k "test_db"`
- Use `--tb=short` for shorter tracebacks: `pytest tests/ --tb=short`
- Use `-x` to stop on first failure: `pytest tests/ -x`

### Continuous Integration

Tests are designed to run in CI/CD pipelines:

```bash
# CI-friendly command (no interactive output)
pytest tests/ --cov=src/shortstory --cov-report=xml --cov-report=term --junitxml=test-results.xml
```

## Example Test Story

Try this story idea:
- **Idea:** "A lighthouse keeper who collects lost voices in glass jars"
- **Character:** Mara, a lighthouse keeper with an unusual collection
- **Theme:** "What happens to the stories we never tell?"
- **Genre:** Literary

This should generate a distinctive, memorable short story!

